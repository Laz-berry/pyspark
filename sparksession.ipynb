{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a125117-52c7-4f51-846a-18e7106eaa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://joon:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eb71f04760>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "# spark 객체 생성    \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afe413-6c2a-43c8-9ed7-c963d305a0cf",
   "metadata": {},
   "source": [
    "### Create json file using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e799237d-a750-4639-b409-0b45f64a3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sparkContext로 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# json file 읽어들이기\n",
    "path ='people.json'\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# printSchema()로 json파일의 스키마 형태 볼 수 있음\n",
    "peopleDF.printSchema()\n",
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a974ce-9d49-4532-9387-f10e4a23b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임을 사용하는 임시의 view(가상의 테이블) 생성\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# spark에서 제공하는 sql 메소드를 이용해 쿼리 날리기\n",
    "# 쿼리문에서 people 테이블은 위에서 만들었던 view 테이블!\n",
    "teenagerNameDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNameDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdc7cbf-fce5-4d5d-89fe-b5f9a60db434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일 읽어들이기\n",
    "path = 'people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# Global Temporary View 생성\n",
    "df.createOrReplaceGlobalTempView('people')\n",
    "\n",
    "# 'global_temp' 키워드를 사용해야 모든 SparkSession들 간에 View를 공유가능\n",
    "sqlDF = spark.sql('SELECT * FROM global_temp.people')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0560db8e-becf-4808-b56c-f8b3dab6e0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|{Columbus, Ohio}| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임은 RDD[String] 자료구조를 이용해서 json 데이터셋을\n",
    "# 데이터 프레임으로 생성 가능\n",
    "jsonStrings = ['{\"name\": \"Yin\", \"address\":{\"city\":\"Columbus\", \"state\":\"Ohio\"}}']\n",
    "\n",
    "# json --> RDD형식으로 만들기\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "\n",
    "# json파일 읽어오기\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.printSchema()\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7953c43-649a-4b90-8d85-e56798ef208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark 내부에서의 Dataset과 Dataframe의 차이\n",
    "# Dataset : 받아오는 데이터의 형태를 미리 정의해 놓음\n",
    "# DataFrame : 프로그램이 데이터의 형태를 추측함\n",
    "\n",
    "# DataFrame의 칼럼에 접근하기 위한 방법 2가지 : df.column, df\\['column''\\]\n",
    "\n",
    "# json 파일 읽어들이기\n",
    "path = 'people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# name 칼럼 select해서 살펴보기\n",
    "df.select('name').show()\n",
    "\n",
    "# name 칼럼 + 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "\n",
    "# age가 20보다 큰 데이터만 추출\n",
    "df.filter(df['age'] > 20).show()\n",
    "\n",
    "# age 칼럼으로 grouping 데이터의 개수를 집계\n",
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698b21a3-9c0e-496a-b22c-65fa884b3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Michel| 29|\n",
      "|  Andy| 30|\n",
      "|Juston| 19|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# SparkContext 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# txt file 읽기\n",
    "lines = sc.textFile('people.txt')\n",
    "parts = lines.map(lambda x: x.split(','))\n",
    "\n",
    "## step 1 ## --> value 처리\n",
    "# 각 라인을 tuple( , ) 형태로 convert\n",
    "people = parts.map(lambda y: (y[0], y[1].strip())) # name에서 공백 strip\n",
    "\n",
    "## step 2 ## --> Schema들 처리\n",
    "# 문자열로 인코딩된 스키마\n",
    "schemaString = \"name age\"\n",
    "\n",
    "# schemaString 요소를 loop돌면서 StructField로 만들기\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "\n",
    "# StructField 여러 개가 있는 리스트를 StructType으로 만들기\n",
    "schema = StructType(fields)\n",
    "\n",
    "## step 3 ## --> value와 schema 활용해 DataFrame 생성\n",
    "# 위에서 만든 schema를 RDD의 schema로 적용\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# View Table 생성해 쿼리 날려서 데이터 추출해보기\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "results = spark.sql(\"SELECT * FROM people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161080f-3a40-40c5-96d7-08928846f3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

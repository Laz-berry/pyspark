{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dcb4f5d-ade3-43f9-8fa0-9b6a20b755e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be0f5f7-e3ce-4fe9-b554-bda191af5afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://joon:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79909393-84a9-4561-9645-069a4873e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of RangeRDD: <class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "data = range(1, 101)\n",
    "\n",
    "# 해당 데이터를 8개의 파티션으로 나눔\n",
    "rangeRDD = sc.parallelize(data, 8)\n",
    "\n",
    "# 해당 RDD 타입 확인\n",
    "print('type of RangeRDD: {0}'.format(type(rangeRDD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d51b0b5-79a1-44de-95c5-b3c166c2b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T] method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using range\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    .. versionadded:: 0.7.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    c : :class:`collections.abc.Iterable`\n",
      "        iterable collection to distribute\n",
      "    numSlices : int, optional\n",
      "        the number of partitions of the new RDD\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`RDD`\n",
      "        RDD representing distributed collection.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "    \n",
      "    Deal with a list of strings.\n",
      "    \n",
      "    >>> strings = [\"a\", \"b\", \"c\"]\n",
      "    >>> sc.parallelize(strings, 2).glom().collect()\n",
      "    [['a'], ['b', 'c']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1dbb9d-301e-45e6-9981-6ffd65e543de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD의 파티션 숫자 확인\n",
    "rangeRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1223d09-6ae0-4f7f-8b87-ff3fa3aa7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) PythonRDD[4] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:287 []'\n",
      "rangeRDD id: 4\n"
     ]
    }
   ],
   "source": [
    "print(rangeRDD.toDebugString())\n",
    "\n",
    "print('rangeRDD id: {0}'.format(rangeRDD.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5b914b2-2b1b-4943-ada4-b092f4668a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) My first RDD PythonRDD[4] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:287 []'\n"
     ]
    }
   ],
   "source": [
    "rangeRDD.setName('My first RDD')\n",
    "\n",
    "# RDD에 이름 지정\n",
    "print(rangeRDD.toDebugString())\n",
    "# help(rangeRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92ad12-466d-4378-bcf1-20e5f89f1a99",
   "metadata": {},
   "source": [
    "#### map transformation이 적용되면 base RDD에 있는 각각의 item은 새로운 RDD로 재탄생함. base RDD가 20개의 elements를 가지고 있다면 새로운 RDD 역시 20개의 elements를 가짐.\n",
    "\n",
    "##### spark는 lazy evaluation 방식이라 transformation이 발생해도 실제 데이터는 변경되지 않음.\n",
    "##### collect() 함수를 통해서 분배된 데이터들을 새로운 list로 합치고 transformation도 적용.\n",
    "##### 주의! collect()함수는 적은 양의 데이터 일 경우에만 사용함!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51589d5b-93c9-49ee-9b92-5f1587408504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "def sub(value):\n",
    "    return (value - 1)\n",
    "\n",
    "# RDD에 sub함수를 적용시킴\n",
    "subRDD = rangeRDD.map(sub)\n",
    "\n",
    "# 실제 action이 이루어지고 sub함수가 적용된 리스트 리턴\n",
    "print(subRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c98e3b7-e5cc-490f-87ff-575e6cac1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# filter() : 함수 결과가 참인 경우에만 요소들을 통과시킴. \n",
    "# 결과로 새로운 RDD를 생성. action 아님\n",
    "\n",
    "def ten(value):\n",
    "    if(value < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "filteredRDD = subRDD.filter(ten)\n",
    "print(filteredRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15c935ef-a3b8-4437-b38d-5ca603fc1828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Lambda() : 런타인에서 이름을 할당 받을 필요가 없는 한 줄 짜리 익명함수\n",
    "\n",
    "lambdaRDD = subRDD.filter(lambda x:x < 10)\n",
    "print(lambdaRDD.collect())\n",
    "\n",
    "evenRDD = lambdaRDD.filter(lambda x: x%2 == 0)\n",
    "print(evenRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc9a6604-17fd-44a3-9a10-b99046988655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 1, 2, 3]\n",
      "\n",
      "[9, 8, 7, 6]\n",
      "[0, 1, 2, 3]\n",
      "\n",
      "45\n",
      "45\n",
      "-45\n",
      "-45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(filteredRDD.first())\n",
    "print(filteredRDD.take(4))\n",
    "print()\n",
    "\n",
    "# 해당 개수에 데이터를 정렬을 해서 가져옴(오름, 내림차순)\n",
    "print(filteredRDD.takeOrdered(4, lambda s:-s))\n",
    "print(filteredRDD.top(4, lambda s:-s))\n",
    "print()\n",
    "\n",
    "from operator import add\n",
    "print(filteredRDD.reduce(add))\n",
    "print(filteredRDD.reduce(lambda a,b : a+b))\n",
    "print(filteredRDD.reduce(lambda a,b : a-b))\n",
    "print(filteredRDD.repartition(4).reduce(lambda a,b : a-b))\n",
    "print(filteredRDD.repartition(4).reduce(lambda a,b : a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d534a8-05a6-4922-8d18-655d640e196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 6, 4, 8, 6]\n",
      "defaultdict(<class 'int'>, {1: 4, 2: 4, 3: 5, 4: 2, 5: 1, 6: 1})\n"
     ]
    }
   ],
   "source": [
    "# takeSample() : dataset으로부터 랜덤으로 원소를 리턴한다.\n",
    "# withReplacement 파라미터가 있는데 True일 경우 동일한 원소가 여러번 리턴가능\n",
    "# seed 파라미터는 랜덤 넘버를 생성할 때 seed값으로 설정, action이기 때문에 실제 메모리에서 계산\n",
    "\n",
    "print(filteredRDD.takeSample(withReplacement=True, num=6, seed=500))\n",
    "repetitiveRDD = sc.parallelize([1,2,3,1,2,3,1,2,1,2,3,3,3,4,5,4,6])\n",
    "\n",
    "# 값으로 그룹화 한 후 그 개수를 count함\n",
    "print(repetitiveRDD.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463763b8-4dd3-4d3f-b7a8-214a4b1f8fe2",
   "metadata": {},
   "source": [
    "##### flatmap() : map()이 적용된 RDD는 iterator로 만들어진 새로운 RDD를 얻는데, iterator안에 포함된 값으로 RDD를 구성하기 원할 경우에 flatmap()을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "808b3c80-72cf-4fc4-8718-21fb9613ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[63] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "wordsRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0529375b-6198-4be7-850e-68fa1cf4b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'cats'), ('elephant', 'elephants'), ('rat', 'rats'), ('rat', 'rats'), ('cat', 'cats')]\n"
     ]
    }
   ],
   "source": [
    "# map()을 수행한 결과가 collection, 튜플로 리턴\n",
    "\n",
    "wordsRDDMap = wordsRDD.map(lambda x: (x, x+'s'))\n",
    "print(wordsRDDMap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f05a962e-1545-428e-84b2-93b4ace919e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cats', 'elephant', 'elephants', 'rat', 'rats', 'rat', 'rats', 'cat', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# flatmap()을 수행한 결과 iteraor 안에 포함된 값으로 리턴\n",
    "\n",
    "wordsRDDMap = wordsRDD.flatMap(lambda x: (x, x+'s'))\n",
    "print(wordsRDDMap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65130c2b-13f9-409e-9d7e-a0ec09cc47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', [1]), ('a', [1, 2])]\n",
      "[('b', 1.0), ('a', 3.0)]\n",
      "[('b', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey() : pair RDD로 이루어져 있는 경우에 적용가능, 대규모 분산 dataset에 대해 매우 효과적임\n",
    "# node를 통해 data가 셔플이 일어나기 전에 각각 파티션에서 키를 통해 출력 데이터를 결합할 수 있기 때문.\n",
    "# ex) [(a, 1), (b, 2)] --> 키는 튜플의 첫 번째 원소, 값은 두 번째 원소\n",
    "\n",
    "# groupByKey() : 모든 key-value 쌍이 셔플함. (spark는 메모리가 차면 disk로 자동 swap하지만 일괄적으로 동작,\n",
    "# 따라서 out-of-memory가 발생 할 수 있음)\n",
    "\n",
    "import math\n",
    "\n",
    "pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "print(pairRDD.groupByKey().mapValues(lambda x: list(x)).collect())\n",
    "print(pairRDD.groupByKey().mapValues(lambda x: math.fsum(x)).collect())\n",
    "print(pairRDD.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90134475-9b61-4bf2-b41e-127ee9e304d1",
   "metadata": {},
   "source": [
    "##### 효율적인 RDD 메모리 관리를 위해선 컨텐츠를 메모리에 보관하는게 유리하다. 하지만 너무 많으면 spark에서는 자동적으로 RDD를 삭제\n",
    "##### cache() 함수를 통해 만들어진 RDD를 메모리에 상주 시킬 수 있다. 가장 적은 빈도로 사용했던(LRU) RDD부터 삭제를 시작함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66c6c412-d445-4808-9117-b2082f374813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "filteredRDD.setName('My Filtered RDD')\n",
    "filteredRDD.cache()\n",
    "print(filteredRDD.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1997bd5-ffce-442e-b1f7-9c774a50eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) My Filtered RDD PythonRDD[6] at collect at C:\\\\Users\\\\qkrwn\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17140\\\\2634476502.py:11 [Memory Serialized 1x Replicated]\\n |  ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:287 [Memory Serialized 1x Replicated]'\n",
      "\n",
      "Serialized 1x Replicated\n",
      "\n",
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# toDebugString() : 현재 생성된 RDD 정보를 보여줌\n",
    "\n",
    "print(filteredRDD.toDebugString())\n",
    "filteredRDD.unpersist()\n",
    "print()\n",
    "\n",
    "# getStorageLevel() : RDD가 현재 어느 위치에 저장되어 있는지(mem or disk)\n",
    "print(filteredRDD.getStorageLevel())\n",
    "filteredRDD.cache()\n",
    "print()\n",
    "\n",
    "print(filteredRDD.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e540539-ffd0-40f4-80ee-eef0482adcef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
